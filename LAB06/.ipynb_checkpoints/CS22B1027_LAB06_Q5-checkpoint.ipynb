{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Defined Functions and Classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes, self.class_counts = np.unique(y_train, return_counts=True)\n",
    "        self.no_of_classes = len(self.classes)\n",
    "        self.total_data_points = len(y_train)\n",
    "        self.apriori_probabilities = self.class_counts/self.total_data_points\n",
    "        self.class_split_training_data = {}\n",
    "        for c in self.classes:\n",
    "            self.class_split_training_data[c] = []\n",
    "            for j in range(len(y_train)):\n",
    "                if c == y_train[j]:\n",
    "                    self.class_split_training_data[c].append(X_train[j])\n",
    "            self.class_split_training_data[c] = np.array(self.class_split_training_data[c])\n",
    "        self.mean_vectors = []\n",
    "        for c in self.classes:\n",
    "            self.mean_vectors.append(np.mean(self.class_split_training_data[c], axis = 0))\n",
    "        self.covariance_matrices = []\n",
    "        for c in self.classes:\n",
    "            self.covariance_matrices.append(np.cov(self.class_split_training_data[c], rowvar=False))\n",
    "        self.inverse_covariance_matrices = []\n",
    "        for c in range(len(self.classes)):\n",
    "            self.inverse_covariance_matrices.append(np.linalg.inv(self.covariance_matrices[c]))\n",
    "        self.dimensions = len(X_train[0])\n",
    "        self.det_covariance_matrices = []\n",
    "        for c in range(len(self.classes)):\n",
    "            self.det_covariance_matrices.append(sp.Matrix(self.covariance_matrices[c]).det())\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for X in X_test:\n",
    "            probability_values = {}\n",
    "            class_count = 0\n",
    "            for c in self.classes:\n",
    "                p_xw = np.exp(-0.5 * np.dot(np.dot((X - self.mean_vectors[class_count]).T, self.inverse_covariance_matrices[class_count]), (X - self.mean_vectors[class_count]))) / (((2 * np.pi) ** (self.dimensions / 2)) * np.power(self.det_covariance_matrices[c], 0.5))\n",
    "                p_wx = p_xw * self.apriori_probabilities[class_count]\n",
    "                probability_values[c] = p_wx\n",
    "                class_count += 1\n",
    "            y_pred.append(max(probability_values, key=probability_values.get))\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "     Unnamed: 0 Unnamed: 1         0         1         2         3         4  \\\n0             1       male -0.066420  0.151611  0.027740  0.052771 -0.066105   \n1             2       male -0.030614  0.049667  0.008084 -0.050324  0.007649   \n2             3       male -0.096178  0.061127  0.035326 -0.035388 -0.090728   \n3             4       male -0.103057  0.085044  0.078333 -0.035873 -0.028163   \n4             5       male -0.125815  0.120046  0.023131 -0.042901  0.038215   \n..          ...        ...       ...       ...       ...       ...       ...   \n795         796     female -0.164731  0.064301  0.058630 -0.017420 -0.157600   \n796         797     female -0.095308  0.051095  0.092913 -0.101745 -0.083153   \n797         798     female -0.202852  0.037039  0.079731 -0.047156 -0.140062   \n798         799     female -0.088300  0.063530  0.049627 -0.026011 -0.172773   \n799         800     female -0.156201  0.055165  0.142716 -0.115393 -0.128982   \n\n            5         6         7  ...       118       119       120  \\\n0   -0.041232 -0.002637 -0.158467  ...  0.025989 -0.001087  0.027260   \n1   -0.063818 -0.019530 -0.119905  ...  0.044229 -0.023900 -0.028108   \n2   -0.018634 -0.024315 -0.139786  ...  0.111141  0.059436 -0.029222   \n3    0.004924  0.007829 -0.017016  ...  0.100793 -0.002644 -0.023388   \n4   -0.049677 -0.054258 -0.130758  ...  0.090197  0.067527  0.039926   \n..        ...       ...       ...  ...       ...       ...       ...   \n795 -0.022536  0.002864 -0.072739  ...  0.095115  0.007198 -0.004655   \n796 -0.028159  0.009090 -0.114513  ...  0.056078  0.119846  0.087470   \n797 -0.080246  0.057668 -0.122083  ...  0.066954  0.035684 -0.023112   \n798  0.086218  0.042710 -0.161852  ...  0.039460  0.067547  0.040426   \n799 -0.139830 -0.037305 -0.101402  ...  0.024955  0.066980 -0.002332   \n\n          121       122       123       124       125       126       127  \n0   -0.046754 -0.118619 -0.163774 -0.000590 -0.076400  0.107497  0.001567  \n1    0.040618 -0.146579 -0.141244  0.016162  0.017638  0.080610 -0.015930  \n2    0.042115 -0.222173 -0.116908  0.093428  0.017391  0.057652  0.086116  \n3    0.029497 -0.139830 -0.119243  0.005306 -0.015100  0.161575  0.062462  \n4    0.047469 -0.056852 -0.076700  0.004966  0.028171  0.026041  0.084135  \n..        ...       ...       ...       ...       ...       ...       ...  \n795  0.023957 -0.170753 -0.136630  0.041614  0.031600  0.019064  0.004384  \n796  0.017481 -0.096594 -0.084553  0.037709  0.030732 -0.083713  0.064970  \n797 -0.030452 -0.154243 -0.188270  0.071086  0.037384 -0.006257  0.039977  \n798  0.028007 -0.154515 -0.127736  0.046967  0.009701 -0.016942  0.048071  \n799 -0.045738 -0.110557 -0.014995 -0.002124 -0.010298 -0.028856  0.075323  \n\n[800 rows x 130 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>118</th>\n      <th>119</th>\n      <th>120</th>\n      <th>121</th>\n      <th>122</th>\n      <th>123</th>\n      <th>124</th>\n      <th>125</th>\n      <th>126</th>\n      <th>127</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>male</td>\n      <td>-0.066420</td>\n      <td>0.151611</td>\n      <td>0.027740</td>\n      <td>0.052771</td>\n      <td>-0.066105</td>\n      <td>-0.041232</td>\n      <td>-0.002637</td>\n      <td>-0.158467</td>\n      <td>...</td>\n      <td>0.025989</td>\n      <td>-0.001087</td>\n      <td>0.027260</td>\n      <td>-0.046754</td>\n      <td>-0.118619</td>\n      <td>-0.163774</td>\n      <td>-0.000590</td>\n      <td>-0.076400</td>\n      <td>0.107497</td>\n      <td>0.001567</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>male</td>\n      <td>-0.030614</td>\n      <td>0.049667</td>\n      <td>0.008084</td>\n      <td>-0.050324</td>\n      <td>0.007649</td>\n      <td>-0.063818</td>\n      <td>-0.019530</td>\n      <td>-0.119905</td>\n      <td>...</td>\n      <td>0.044229</td>\n      <td>-0.023900</td>\n      <td>-0.028108</td>\n      <td>0.040618</td>\n      <td>-0.146579</td>\n      <td>-0.141244</td>\n      <td>0.016162</td>\n      <td>0.017638</td>\n      <td>0.080610</td>\n      <td>-0.015930</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>male</td>\n      <td>-0.096178</td>\n      <td>0.061127</td>\n      <td>0.035326</td>\n      <td>-0.035388</td>\n      <td>-0.090728</td>\n      <td>-0.018634</td>\n      <td>-0.024315</td>\n      <td>-0.139786</td>\n      <td>...</td>\n      <td>0.111141</td>\n      <td>0.059436</td>\n      <td>-0.029222</td>\n      <td>0.042115</td>\n      <td>-0.222173</td>\n      <td>-0.116908</td>\n      <td>0.093428</td>\n      <td>0.017391</td>\n      <td>0.057652</td>\n      <td>0.086116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>male</td>\n      <td>-0.103057</td>\n      <td>0.085044</td>\n      <td>0.078333</td>\n      <td>-0.035873</td>\n      <td>-0.028163</td>\n      <td>0.004924</td>\n      <td>0.007829</td>\n      <td>-0.017016</td>\n      <td>...</td>\n      <td>0.100793</td>\n      <td>-0.002644</td>\n      <td>-0.023388</td>\n      <td>0.029497</td>\n      <td>-0.139830</td>\n      <td>-0.119243</td>\n      <td>0.005306</td>\n      <td>-0.015100</td>\n      <td>0.161575</td>\n      <td>0.062462</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>male</td>\n      <td>-0.125815</td>\n      <td>0.120046</td>\n      <td>0.023131</td>\n      <td>-0.042901</td>\n      <td>0.038215</td>\n      <td>-0.049677</td>\n      <td>-0.054258</td>\n      <td>-0.130758</td>\n      <td>...</td>\n      <td>0.090197</td>\n      <td>0.067527</td>\n      <td>0.039926</td>\n      <td>0.047469</td>\n      <td>-0.056852</td>\n      <td>-0.076700</td>\n      <td>0.004966</td>\n      <td>0.028171</td>\n      <td>0.026041</td>\n      <td>0.084135</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>796</td>\n      <td>female</td>\n      <td>-0.164731</td>\n      <td>0.064301</td>\n      <td>0.058630</td>\n      <td>-0.017420</td>\n      <td>-0.157600</td>\n      <td>-0.022536</td>\n      <td>0.002864</td>\n      <td>-0.072739</td>\n      <td>...</td>\n      <td>0.095115</td>\n      <td>0.007198</td>\n      <td>-0.004655</td>\n      <td>0.023957</td>\n      <td>-0.170753</td>\n      <td>-0.136630</td>\n      <td>0.041614</td>\n      <td>0.031600</td>\n      <td>0.019064</td>\n      <td>0.004384</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>797</td>\n      <td>female</td>\n      <td>-0.095308</td>\n      <td>0.051095</td>\n      <td>0.092913</td>\n      <td>-0.101745</td>\n      <td>-0.083153</td>\n      <td>-0.028159</td>\n      <td>0.009090</td>\n      <td>-0.114513</td>\n      <td>...</td>\n      <td>0.056078</td>\n      <td>0.119846</td>\n      <td>0.087470</td>\n      <td>0.017481</td>\n      <td>-0.096594</td>\n      <td>-0.084553</td>\n      <td>0.037709</td>\n      <td>0.030732</td>\n      <td>-0.083713</td>\n      <td>0.064970</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>798</td>\n      <td>female</td>\n      <td>-0.202852</td>\n      <td>0.037039</td>\n      <td>0.079731</td>\n      <td>-0.047156</td>\n      <td>-0.140062</td>\n      <td>-0.080246</td>\n      <td>0.057668</td>\n      <td>-0.122083</td>\n      <td>...</td>\n      <td>0.066954</td>\n      <td>0.035684</td>\n      <td>-0.023112</td>\n      <td>-0.030452</td>\n      <td>-0.154243</td>\n      <td>-0.188270</td>\n      <td>0.071086</td>\n      <td>0.037384</td>\n      <td>-0.006257</td>\n      <td>0.039977</td>\n    </tr>\n    <tr>\n      <th>798</th>\n      <td>799</td>\n      <td>female</td>\n      <td>-0.088300</td>\n      <td>0.063530</td>\n      <td>0.049627</td>\n      <td>-0.026011</td>\n      <td>-0.172773</td>\n      <td>0.086218</td>\n      <td>0.042710</td>\n      <td>-0.161852</td>\n      <td>...</td>\n      <td>0.039460</td>\n      <td>0.067547</td>\n      <td>0.040426</td>\n      <td>0.028007</td>\n      <td>-0.154515</td>\n      <td>-0.127736</td>\n      <td>0.046967</td>\n      <td>0.009701</td>\n      <td>-0.016942</td>\n      <td>0.048071</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>800</td>\n      <td>female</td>\n      <td>-0.156201</td>\n      <td>0.055165</td>\n      <td>0.142716</td>\n      <td>-0.115393</td>\n      <td>-0.128982</td>\n      <td>-0.139830</td>\n      <td>-0.037305</td>\n      <td>-0.101402</td>\n      <td>...</td>\n      <td>0.024955</td>\n      <td>0.066980</td>\n      <td>-0.002332</td>\n      <td>-0.045738</td>\n      <td>-0.110557</td>\n      <td>-0.014995</td>\n      <td>-0.002124</td>\n      <td>-0.010298</td>\n      <td>-0.028856</td>\n      <td>0.075323</td>\n    </tr>\n  </tbody>\n</table>\n<p>800 rows × 130 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('face feature vectors.csv')\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Train Split of Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "types = dataset.iloc[:, 1].unique()\n",
    "test_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "for t in types:\n",
    "    type_df = dataset[dataset.iloc[:, 1] == t]\n",
    "    train_df = pd.concat([train_df, type_df.iloc[5:, :]])\n",
    "    test_df = pd.concat([test_df, type_df.iloc[:5, :]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "     Unnamed: 0 Unnamed: 1         0         1         2         3         4  \\\n0             1       male -0.066420  0.151611  0.027740  0.052771 -0.066105   \n1             2       male -0.030614  0.049667  0.008084 -0.050324  0.007649   \n2             3       male -0.096178  0.061127  0.035326 -0.035388 -0.090728   \n3             4       male -0.103057  0.085044  0.078333 -0.035873 -0.028163   \n4             5       male -0.125815  0.120046  0.023131 -0.042901  0.038215   \n400         401     female  0.001747  0.185678  0.073260  0.042142 -0.088674   \n401         402     female -0.091598  0.095340  0.072125 -0.092276 -0.079953   \n402         403     female -0.018751  0.088572  0.068894 -0.065700 -0.115126   \n403         404     female -0.130889  0.093262  0.122244 -0.110014 -0.157625   \n404         405     female -0.037433  0.078158  0.118061 -0.117658 -0.194807   \n\n            5         6         7  ...       118       119       120  \\\n0   -0.041232 -0.002637 -0.158467  ...  0.025989 -0.001087  0.027260   \n1   -0.063818 -0.019530 -0.119905  ...  0.044229 -0.023900 -0.028108   \n2   -0.018634 -0.024315 -0.139786  ...  0.111141  0.059436 -0.029222   \n3    0.004924  0.007829 -0.017016  ...  0.100793 -0.002644 -0.023388   \n4   -0.049677 -0.054258 -0.130758  ...  0.090197  0.067527  0.039926   \n400  0.028186 -0.027830 -0.064211  ...  0.123615  0.030036  0.041442   \n401  0.047782 -0.004701 -0.092005  ...  0.011370  0.144719  0.089139   \n402  0.024339 -0.028420 -0.159320  ...  0.010345  0.095309  0.012255   \n403 -0.036781  0.073908 -0.098571  ...  0.003229  0.049330  0.059733   \n404 -0.045464 -0.014104 -0.158824  ...  0.043848 -0.009760  0.043486   \n\n          121       122       123       124       125       126       127  \n0   -0.046754 -0.118619 -0.163774 -0.000590 -0.076400  0.107497  0.001567  \n1    0.040618 -0.146579 -0.141244  0.016162  0.017638  0.080610 -0.015930  \n2    0.042115 -0.222173 -0.116908  0.093428  0.017391  0.057652  0.086116  \n3    0.029497 -0.139830 -0.119243  0.005306 -0.015100  0.161575  0.062462  \n4    0.047469 -0.056852 -0.076700  0.004966  0.028171  0.026041  0.084135  \n400 -0.012818 -0.119177 -0.165786 -0.075368 -0.017690  0.067028  0.036452  \n401 -0.059767 -0.111235 -0.055420  0.006283  0.016900 -0.081676  0.022809  \n402 -0.033010 -0.097720 -0.218889  0.077764  0.045943  0.010856  0.100522  \n403 -0.023820 -0.098432 -0.034316  0.075131 -0.029204 -0.020707  0.031028  \n404  0.020251 -0.115420 -0.040023  0.099409 -0.032240  0.037601 -0.020016  \n\n[10 rows x 130 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>118</th>\n      <th>119</th>\n      <th>120</th>\n      <th>121</th>\n      <th>122</th>\n      <th>123</th>\n      <th>124</th>\n      <th>125</th>\n      <th>126</th>\n      <th>127</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>male</td>\n      <td>-0.066420</td>\n      <td>0.151611</td>\n      <td>0.027740</td>\n      <td>0.052771</td>\n      <td>-0.066105</td>\n      <td>-0.041232</td>\n      <td>-0.002637</td>\n      <td>-0.158467</td>\n      <td>...</td>\n      <td>0.025989</td>\n      <td>-0.001087</td>\n      <td>0.027260</td>\n      <td>-0.046754</td>\n      <td>-0.118619</td>\n      <td>-0.163774</td>\n      <td>-0.000590</td>\n      <td>-0.076400</td>\n      <td>0.107497</td>\n      <td>0.001567</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>male</td>\n      <td>-0.030614</td>\n      <td>0.049667</td>\n      <td>0.008084</td>\n      <td>-0.050324</td>\n      <td>0.007649</td>\n      <td>-0.063818</td>\n      <td>-0.019530</td>\n      <td>-0.119905</td>\n      <td>...</td>\n      <td>0.044229</td>\n      <td>-0.023900</td>\n      <td>-0.028108</td>\n      <td>0.040618</td>\n      <td>-0.146579</td>\n      <td>-0.141244</td>\n      <td>0.016162</td>\n      <td>0.017638</td>\n      <td>0.080610</td>\n      <td>-0.015930</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>male</td>\n      <td>-0.096178</td>\n      <td>0.061127</td>\n      <td>0.035326</td>\n      <td>-0.035388</td>\n      <td>-0.090728</td>\n      <td>-0.018634</td>\n      <td>-0.024315</td>\n      <td>-0.139786</td>\n      <td>...</td>\n      <td>0.111141</td>\n      <td>0.059436</td>\n      <td>-0.029222</td>\n      <td>0.042115</td>\n      <td>-0.222173</td>\n      <td>-0.116908</td>\n      <td>0.093428</td>\n      <td>0.017391</td>\n      <td>0.057652</td>\n      <td>0.086116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>male</td>\n      <td>-0.103057</td>\n      <td>0.085044</td>\n      <td>0.078333</td>\n      <td>-0.035873</td>\n      <td>-0.028163</td>\n      <td>0.004924</td>\n      <td>0.007829</td>\n      <td>-0.017016</td>\n      <td>...</td>\n      <td>0.100793</td>\n      <td>-0.002644</td>\n      <td>-0.023388</td>\n      <td>0.029497</td>\n      <td>-0.139830</td>\n      <td>-0.119243</td>\n      <td>0.005306</td>\n      <td>-0.015100</td>\n      <td>0.161575</td>\n      <td>0.062462</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>male</td>\n      <td>-0.125815</td>\n      <td>0.120046</td>\n      <td>0.023131</td>\n      <td>-0.042901</td>\n      <td>0.038215</td>\n      <td>-0.049677</td>\n      <td>-0.054258</td>\n      <td>-0.130758</td>\n      <td>...</td>\n      <td>0.090197</td>\n      <td>0.067527</td>\n      <td>0.039926</td>\n      <td>0.047469</td>\n      <td>-0.056852</td>\n      <td>-0.076700</td>\n      <td>0.004966</td>\n      <td>0.028171</td>\n      <td>0.026041</td>\n      <td>0.084135</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>401</td>\n      <td>female</td>\n      <td>0.001747</td>\n      <td>0.185678</td>\n      <td>0.073260</td>\n      <td>0.042142</td>\n      <td>-0.088674</td>\n      <td>0.028186</td>\n      <td>-0.027830</td>\n      <td>-0.064211</td>\n      <td>...</td>\n      <td>0.123615</td>\n      <td>0.030036</td>\n      <td>0.041442</td>\n      <td>-0.012818</td>\n      <td>-0.119177</td>\n      <td>-0.165786</td>\n      <td>-0.075368</td>\n      <td>-0.017690</td>\n      <td>0.067028</td>\n      <td>0.036452</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>402</td>\n      <td>female</td>\n      <td>-0.091598</td>\n      <td>0.095340</td>\n      <td>0.072125</td>\n      <td>-0.092276</td>\n      <td>-0.079953</td>\n      <td>0.047782</td>\n      <td>-0.004701</td>\n      <td>-0.092005</td>\n      <td>...</td>\n      <td>0.011370</td>\n      <td>0.144719</td>\n      <td>0.089139</td>\n      <td>-0.059767</td>\n      <td>-0.111235</td>\n      <td>-0.055420</td>\n      <td>0.006283</td>\n      <td>0.016900</td>\n      <td>-0.081676</td>\n      <td>0.022809</td>\n    </tr>\n    <tr>\n      <th>402</th>\n      <td>403</td>\n      <td>female</td>\n      <td>-0.018751</td>\n      <td>0.088572</td>\n      <td>0.068894</td>\n      <td>-0.065700</td>\n      <td>-0.115126</td>\n      <td>0.024339</td>\n      <td>-0.028420</td>\n      <td>-0.159320</td>\n      <td>...</td>\n      <td>0.010345</td>\n      <td>0.095309</td>\n      <td>0.012255</td>\n      <td>-0.033010</td>\n      <td>-0.097720</td>\n      <td>-0.218889</td>\n      <td>0.077764</td>\n      <td>0.045943</td>\n      <td>0.010856</td>\n      <td>0.100522</td>\n    </tr>\n    <tr>\n      <th>403</th>\n      <td>404</td>\n      <td>female</td>\n      <td>-0.130889</td>\n      <td>0.093262</td>\n      <td>0.122244</td>\n      <td>-0.110014</td>\n      <td>-0.157625</td>\n      <td>-0.036781</td>\n      <td>0.073908</td>\n      <td>-0.098571</td>\n      <td>...</td>\n      <td>0.003229</td>\n      <td>0.049330</td>\n      <td>0.059733</td>\n      <td>-0.023820</td>\n      <td>-0.098432</td>\n      <td>-0.034316</td>\n      <td>0.075131</td>\n      <td>-0.029204</td>\n      <td>-0.020707</td>\n      <td>0.031028</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>405</td>\n      <td>female</td>\n      <td>-0.037433</td>\n      <td>0.078158</td>\n      <td>0.118061</td>\n      <td>-0.117658</td>\n      <td>-0.194807</td>\n      <td>-0.045464</td>\n      <td>-0.014104</td>\n      <td>-0.158824</td>\n      <td>...</td>\n      <td>0.043848</td>\n      <td>-0.009760</td>\n      <td>0.043486</td>\n      <td>0.020251</td>\n      <td>-0.115420</td>\n      <td>-0.040023</td>\n      <td>0.099409</td>\n      <td>-0.032240</td>\n      <td>0.037601</td>\n      <td>-0.020016</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 130 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "     Unnamed: 0 Unnamed: 1         0         1         2         3         4  \\\n5             6       male -0.149119  0.125288  0.142323 -0.009087 -0.031394   \n6             7       male -0.139035  0.073513 -0.001770 -0.034225 -0.101610   \n7             8       male -0.074126 -0.000669  0.004166 -0.082413 -0.096091   \n8             9       male -0.166220  0.042769 -0.031647 -0.036892 -0.143837   \n9            10       male -0.185770  0.154008  0.073184 -0.070829 -0.144617   \n..          ...        ...       ...       ...       ...       ...       ...   \n795         796     female -0.164731  0.064301  0.058630 -0.017420 -0.157600   \n796         797     female -0.095308  0.051095  0.092913 -0.101745 -0.083153   \n797         798     female -0.202852  0.037039  0.079731 -0.047156 -0.140062   \n798         799     female -0.088300  0.063530  0.049627 -0.026011 -0.172773   \n799         800     female -0.156201  0.055165  0.142716 -0.115393 -0.128982   \n\n            5         6         7  ...       118       119       120  \\\n5   -0.123533  0.043598 -0.063999  ...  0.060833  0.089529 -0.034872   \n6    0.065105 -0.014420 -0.054993  ...  0.081007 -0.002164  0.060377   \n7   -0.021992  0.009714 -0.056961  ...  0.050497  0.038932  0.023520   \n8   -0.040566  0.042541 -0.122923  ...  0.014732 -0.049135  0.081770   \n9   -0.019732 -0.019418 -0.004675  ...  0.093317  0.035101 -0.147997   \n..        ...       ...       ...  ...       ...       ...       ...   \n795 -0.022536  0.002864 -0.072739  ...  0.095115  0.007198 -0.004655   \n796 -0.028159  0.009090 -0.114513  ...  0.056078  0.119846  0.087470   \n797 -0.080246  0.057668 -0.122083  ...  0.066954  0.035684 -0.023112   \n798  0.086218  0.042710 -0.161852  ...  0.039460  0.067547  0.040426   \n799 -0.139830 -0.037305 -0.101402  ...  0.024955  0.066980 -0.002332   \n\n          121       122       123       124       125       126       127  \n5    0.057080 -0.137162 -0.072522  0.052731 -0.141460  0.019018  0.085765  \n6    0.080294 -0.139369 -0.150245  0.078657  0.024194  0.062180  0.036039  \n7   -0.090260 -0.147692 -0.008296  0.007609 -0.026687 -0.017523 -0.038310  \n8   -0.027199 -0.096941 -0.094661  0.057797 -0.101063  0.061373  0.062176  \n9   -0.046010 -0.087777 -0.100660  0.036190  0.012158  0.032304  0.085996  \n..        ...       ...       ...       ...       ...       ...       ...  \n795  0.023957 -0.170753 -0.136630  0.041614  0.031600  0.019064  0.004384  \n796  0.017481 -0.096594 -0.084553  0.037709  0.030732 -0.083713  0.064970  \n797 -0.030452 -0.154243 -0.188270  0.071086  0.037384 -0.006257  0.039977  \n798  0.028007 -0.154515 -0.127736  0.046967  0.009701 -0.016942  0.048071  \n799 -0.045738 -0.110557 -0.014995 -0.002124 -0.010298 -0.028856  0.075323  \n\n[790 rows x 130 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Unnamed: 1</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>...</th>\n      <th>118</th>\n      <th>119</th>\n      <th>120</th>\n      <th>121</th>\n      <th>122</th>\n      <th>123</th>\n      <th>124</th>\n      <th>125</th>\n      <th>126</th>\n      <th>127</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>male</td>\n      <td>-0.149119</td>\n      <td>0.125288</td>\n      <td>0.142323</td>\n      <td>-0.009087</td>\n      <td>-0.031394</td>\n      <td>-0.123533</td>\n      <td>0.043598</td>\n      <td>-0.063999</td>\n      <td>...</td>\n      <td>0.060833</td>\n      <td>0.089529</td>\n      <td>-0.034872</td>\n      <td>0.057080</td>\n      <td>-0.137162</td>\n      <td>-0.072522</td>\n      <td>0.052731</td>\n      <td>-0.141460</td>\n      <td>0.019018</td>\n      <td>0.085765</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>male</td>\n      <td>-0.139035</td>\n      <td>0.073513</td>\n      <td>-0.001770</td>\n      <td>-0.034225</td>\n      <td>-0.101610</td>\n      <td>0.065105</td>\n      <td>-0.014420</td>\n      <td>-0.054993</td>\n      <td>...</td>\n      <td>0.081007</td>\n      <td>-0.002164</td>\n      <td>0.060377</td>\n      <td>0.080294</td>\n      <td>-0.139369</td>\n      <td>-0.150245</td>\n      <td>0.078657</td>\n      <td>0.024194</td>\n      <td>0.062180</td>\n      <td>0.036039</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>male</td>\n      <td>-0.074126</td>\n      <td>-0.000669</td>\n      <td>0.004166</td>\n      <td>-0.082413</td>\n      <td>-0.096091</td>\n      <td>-0.021992</td>\n      <td>0.009714</td>\n      <td>-0.056961</td>\n      <td>...</td>\n      <td>0.050497</td>\n      <td>0.038932</td>\n      <td>0.023520</td>\n      <td>-0.090260</td>\n      <td>-0.147692</td>\n      <td>-0.008296</td>\n      <td>0.007609</td>\n      <td>-0.026687</td>\n      <td>-0.017523</td>\n      <td>-0.038310</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>male</td>\n      <td>-0.166220</td>\n      <td>0.042769</td>\n      <td>-0.031647</td>\n      <td>-0.036892</td>\n      <td>-0.143837</td>\n      <td>-0.040566</td>\n      <td>0.042541</td>\n      <td>-0.122923</td>\n      <td>...</td>\n      <td>0.014732</td>\n      <td>-0.049135</td>\n      <td>0.081770</td>\n      <td>-0.027199</td>\n      <td>-0.096941</td>\n      <td>-0.094661</td>\n      <td>0.057797</td>\n      <td>-0.101063</td>\n      <td>0.061373</td>\n      <td>0.062176</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>male</td>\n      <td>-0.185770</td>\n      <td>0.154008</td>\n      <td>0.073184</td>\n      <td>-0.070829</td>\n      <td>-0.144617</td>\n      <td>-0.019732</td>\n      <td>-0.019418</td>\n      <td>-0.004675</td>\n      <td>...</td>\n      <td>0.093317</td>\n      <td>0.035101</td>\n      <td>-0.147997</td>\n      <td>-0.046010</td>\n      <td>-0.087777</td>\n      <td>-0.100660</td>\n      <td>0.036190</td>\n      <td>0.012158</td>\n      <td>0.032304</td>\n      <td>0.085996</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>796</td>\n      <td>female</td>\n      <td>-0.164731</td>\n      <td>0.064301</td>\n      <td>0.058630</td>\n      <td>-0.017420</td>\n      <td>-0.157600</td>\n      <td>-0.022536</td>\n      <td>0.002864</td>\n      <td>-0.072739</td>\n      <td>...</td>\n      <td>0.095115</td>\n      <td>0.007198</td>\n      <td>-0.004655</td>\n      <td>0.023957</td>\n      <td>-0.170753</td>\n      <td>-0.136630</td>\n      <td>0.041614</td>\n      <td>0.031600</td>\n      <td>0.019064</td>\n      <td>0.004384</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>797</td>\n      <td>female</td>\n      <td>-0.095308</td>\n      <td>0.051095</td>\n      <td>0.092913</td>\n      <td>-0.101745</td>\n      <td>-0.083153</td>\n      <td>-0.028159</td>\n      <td>0.009090</td>\n      <td>-0.114513</td>\n      <td>...</td>\n      <td>0.056078</td>\n      <td>0.119846</td>\n      <td>0.087470</td>\n      <td>0.017481</td>\n      <td>-0.096594</td>\n      <td>-0.084553</td>\n      <td>0.037709</td>\n      <td>0.030732</td>\n      <td>-0.083713</td>\n      <td>0.064970</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>798</td>\n      <td>female</td>\n      <td>-0.202852</td>\n      <td>0.037039</td>\n      <td>0.079731</td>\n      <td>-0.047156</td>\n      <td>-0.140062</td>\n      <td>-0.080246</td>\n      <td>0.057668</td>\n      <td>-0.122083</td>\n      <td>...</td>\n      <td>0.066954</td>\n      <td>0.035684</td>\n      <td>-0.023112</td>\n      <td>-0.030452</td>\n      <td>-0.154243</td>\n      <td>-0.188270</td>\n      <td>0.071086</td>\n      <td>0.037384</td>\n      <td>-0.006257</td>\n      <td>0.039977</td>\n    </tr>\n    <tr>\n      <th>798</th>\n      <td>799</td>\n      <td>female</td>\n      <td>-0.088300</td>\n      <td>0.063530</td>\n      <td>0.049627</td>\n      <td>-0.026011</td>\n      <td>-0.172773</td>\n      <td>0.086218</td>\n      <td>0.042710</td>\n      <td>-0.161852</td>\n      <td>...</td>\n      <td>0.039460</td>\n      <td>0.067547</td>\n      <td>0.040426</td>\n      <td>0.028007</td>\n      <td>-0.154515</td>\n      <td>-0.127736</td>\n      <td>0.046967</td>\n      <td>0.009701</td>\n      <td>-0.016942</td>\n      <td>0.048071</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>800</td>\n      <td>female</td>\n      <td>-0.156201</td>\n      <td>0.055165</td>\n      <td>0.142716</td>\n      <td>-0.115393</td>\n      <td>-0.128982</td>\n      <td>-0.139830</td>\n      <td>-0.037305</td>\n      <td>-0.101402</td>\n      <td>...</td>\n      <td>0.024955</td>\n      <td>0.066980</td>\n      <td>-0.002332</td>\n      <td>-0.045738</td>\n      <td>-0.110557</td>\n      <td>-0.014995</td>\n      <td>-0.002124</td>\n      <td>-0.010298</td>\n      <td>-0.028856</td>\n      <td>0.075323</td>\n    </tr>\n  </tbody>\n</table>\n<p>790 rows × 130 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, 2:].values\n",
    "X_test = test_df.iloc[:, 2:].values\n",
    "y_train = train_df.iloc[:, 1].values\n",
    "y_test = test_df.iloc[:, 1].values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14911914  0.1252878   0.14232311 ... -0.1414603   0.01901806\n",
      "   0.08576487]\n",
      " [-0.13903469  0.07351305 -0.00176958 ...  0.02419422  0.06218007\n",
      "   0.03603866]\n",
      " [-0.07412638 -0.00066875  0.00416601 ... -0.02668655 -0.01752269\n",
      "  -0.0383101 ]\n",
      " ...\n",
      " [-0.20285167  0.0370395   0.07973114 ...  0.03738441 -0.00625749\n",
      "   0.03997689]\n",
      " [-0.08829999  0.06353012  0.04962703 ...  0.00970074 -0.01694169\n",
      "   0.04807128]\n",
      " [-0.15620135  0.05516458  0.14271647 ... -0.0102984  -0.02885648\n",
      "   0.0753232 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06641996  0.15161145  0.02773961 ... -0.07640016  0.10749723\n",
      "   0.00156654]\n",
      " [-0.03061386  0.04966652  0.00808374 ...  0.0176384   0.08060966\n",
      "  -0.01592966]\n",
      " [-0.09617768  0.06112669  0.03532604 ...  0.01739147  0.057652\n",
      "   0.08611634]\n",
      " ...\n",
      " [-0.0187513   0.08857222  0.06889394 ...  0.04594309  0.01085567\n",
      "   0.10052187]\n",
      " [-0.13088937  0.09326187  0.12224357 ... -0.02920397 -0.02070727\n",
      "   0.03102766]\n",
      " [-0.03743254  0.07815813  0.11806121 ... -0.03223962  0.03760109\n",
      "  -0.02001636]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male' 'male'\n",
      " 'male' 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female' 'female'\n",
      " 'female' 'female' 'female' 'female' 'female' 'female' 'female']\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'male' 'male' 'male' 'male' 'female' 'female' 'female' 'female'\n",
      " 'female']\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding the Dependent Variable"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "bc = BayesClassifier()\n",
    "bc.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction and Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "y_pred = bc.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "   Actual  Predicted\n0       1          1\n1       1          1\n2       1          1\n3       1          1\n4       1          1\n5       0          1\n6       0          0\n7       0          0\n8       0          0\n9       0          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Actual</th>\n      <th>Predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "final_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of this model is 90.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"The Accuracy of this model is {accuracy*100}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
